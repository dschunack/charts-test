# Subchart Values
monitoring-exporter-infrastructure:
  region: ""
  namespaceName: "cmo-observability"
  clusterName: ""
  accountId: ""
  oidcId: ""
  cloudExporter:
    serviceAccountName: "cloudexporter-sa"

cloud-exporter:
  enabled: true
  aws:
    region: ""
    # comma seperated list of subnetIds to monitor
    monitorSubnetIds: ""
  # if prometheusRules should be created
  alerts:
    create: true
    config: 
      subnetIpsFreeThreshold: "0.1"
  replicaCount: 1
  image:
    repository: ghcr.io/steveizzle/aws-metrics-prom-exporter
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    # tag: "0e201455e3ebda9457ef9bd3b13b798019126794"
  podMonitor:
    create: true
    additionalLabels:
      release: kube-prometheus-stack
  nameOverride: ""
  fullnameOverride: ""
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: "cloudexporter-sa"
  podAnnotations: {}
  podSecurityContext:
    runAsUser: 100
    # fsGroup: 2000
  securityContext:
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 100m
      memory: 64Mi
  nodeSelector:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  tolerations: []
  affinity: {}

helm-exporter:
  enabled: true
  # Specifies which namespaces to query for helm 3 metrics.  Defaults to all
  namespaces: "cmo-observability,cmo-security,cmo-system,cmo-third-party,cmo-namespaces,cert-manager,elastic-system,flux-system,kube-logging,kube-system,kubecost,kyverno,monitoring,strimzi-system,trivy-system"
  # you can specify namespaces to ignore - comma separated list of regexps
  namespacesIgnore: ""

  serviceMonitor:
    # Specifies whether a ServiceMonitor should be created
    create: true
    namespace: cmo-observability
  grafanaDashboard:
    # Specifies whether a Grafana dashboard should be created
    enabled: true
    # Specifies then namespace where the dashboard should be created
    # namespace:
    # Add annotations to Configmap for Grafana dashboard
    dashboardAnnotations: {}
      # k8s-sidecar-target-directory: /tmp/dashboards/myfolder

dcgm-exporter:
  enabled: true
  image:
    repository: nvcr.io/nvidia/k8s/dcgm-exporter
    pullPolicy: IfNotPresent
    # Image tag defaults to AppVersion, but you can use the tag key
    # for the image tag, e.g:
    
  # Change the following reference to "/etc/dcgm-exporter/default-counters.csv"
  # to stop profiling metrics from DCGM
  arguments: ["-f", "/etc/dcgm-exporter/dcp-metrics-included.csv"]
  # NOTE: in general, add any command line arguments to arguments above
  # and they will be passed through.
  # Use "-r", "<HOST>:<PORT>" to connect to an already running hostengine
  # Example arguments: ["-r", "host123:5555"]
  # Use "-n" to remove the hostname tag from the output.
  # Example arguments: ["-n"]
  # Use "-d" to specify the devices to monitor. -d must be followed by a string
  # in the following format: [f] or [g[:numeric_range][+]][i[:numeric_range]]
  # Where a numeric range is something like 0-4 or 0,2,4, etc.
  # Example arguments: ["-d", "g+i"] to monitor all GPUs and GPU instances or
  # ["-d", "g:0-3"] to monitor GPUs 0-3.
  # Use "-m" to specify the namespace and name of a configmap containing
  # the watched exporter fields.
  # Example arguments: ["-m", "default:exporter-metrics-config-map"]

  # Image pull secrets for container images
  imagePullSecrets: []

  # Overrides the chart's name
  nameOverride: ""

  # Overrides the chart's computed fullname
  fullnameOverride: ""

  # Overrides the deployment namespace
  namespaceOverride: ""

  # Defines the runtime class that will be used by the pod
  runtimeClassName: ""
  # Defines serviceAccount names for components.
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  rollingUpdate:
    # Specifies maximum number of DaemonSet pods that can be unavailable during the update
    maxUnavailable: 1
    # Specifies maximum number of nodes with an existing available DaemonSet pod that can have an updated DaemonSet pod during during an update
    maxSurge: 0

  # Labels to be added to dcgm-exporter pods
  podLabels: {}

  # Annotations to be added to dcgm-exporter pods
  podAnnotations: {}
  # Using this annotation which is required for prometheus scraping
    # prometheus.io/scrape: "true"
    # prometheus.io/port: "9400"

  # The SecurityContext for the dcgm-exporter pods
  podSecurityContext: {}
    # fsGroup: 2000

  # The SecurityContext for the dcgm-exporter containers
  securityContext:
    runAsNonRoot: false
    runAsUser: 0
    capabilities:
      add: ["SYS_ADMIN"]
    # readOnlyRootFilesystem: true

  # Defines the dcgm-exporter service
  service:
    # When enabled, the helm chart will create service
    enable: true
    type: ClusterIP
    port: 9400
    address: ":9400"
    # Annotations to add to the service
    annotations: {}

  # Allows to control pod resources
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  serviceMonitor:
    enabled: true
    interval: 15s
    honorLabels: false
    additionalLabels:
      release: monitoring-stack
    relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  nodeSelector:
    kubernetes.io/os: linux

  tolerations:
    - operator: Exists

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          # On discrete-GPU based systems NFD adds the following label where 10de is the NVIDIA PCI vendor ID
          - key: feature.node.kubernetes.io/pci-10de.present
            operator: In
            values:
            - "true"
        - matchExpressions:
          # On some Tegra-based systems NFD detects the CPU vendor ID as NVIDIA
          - key: feature.node.kubernetes.io/cpu-model.vendor_id
            operator: In
            values:
            - "NVIDIA"
        - matchExpressions:
          # We allow a GPU deployment to be forced by setting the following label to "true"
          - key: "nvidia.com/gpu.present"
            operator: In
            values:
            - "true"
        - matchExpressions:
          # We allow a GPU deployment to be forced by setting the following label to "true"
          - key: "karpenter.k8s.aws/instance-gpu-manufacturer"
            operator: In
            values:
            - "nvidia"

  extraHostVolumes: []
  #- name: host-binaries
  #  hostPath: /opt/bin

  extraConfigMapVolumes:
    - name: exporter-metrics-volume
      configMap:
        name: exporter-metrics-config-map
        items:
        - key: metrics
          path: dcp-metrics-included.csv

  extraVolumeMounts:
    - name: exporter-metrics-volume
      mountPath: /etc/dcgm-exporter/dcp-metrics-included.csv
      subPath: dcp-metrics-included.csv

  extraEnv: []
  #- name: EXTRA_VAR
  #  value: "TheStringValue"

  # Path to the kubelet socket for /pod-resources
  kubeletPath: "/var/lib/kubelet/pod-resources"

prometheus-windows-exporter:
  enabled: true
  image:
    registry: ghcr.io
    repository: prometheus-community/windows-exporter
    # Overrides the image tag whose default is {{ printf "v%s" .Chart.AppVersion }}
    tag: ""
    pullPolicy: IfNotPresent
    digest: ""

  config: |-
    collectors:
      enabled: '[defaults],memory,container,cache,tcp'

  imagePullSecrets: []
  # - name: "image-pull-secret"
  nameOverride: ""
  fullnameOverride: ""

  global:
    # To help compatibility with other charts which use global.imagePullSecrets.
    # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style).
    # global:
    #   imagePullSecrets:
    #   - name: pullSecret1
    #   - name: pullSecret2
    # or
    # global:
    #   imagePullSecrets:
    #   - pullSecret1
    #   - pullSecret2
    imagePullSecrets: []
    #
    # Allow parent charts to override registry hostname
    imageRegistry: ""

  service:
    type: ClusterIP
    port: 9182
    nodePort:
    portName: metrics
    annotations: {}

  # Additional environment variables that will be passed to the daemonset
  env: {}
  ##  env:
  ##    VARIABLE: value

  prometheus:
    monitor:
      enabled: false
      additionalLabels: {}
      namespace: ""

      jobLabel: ""

      # List of pod labels to add to windows exporter metrics
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor
      podTargetLabels: []

      scheme: http
      basicAuth: {}
      bearerTokenFile:
      tlsConfig: {}

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## Override serviceMonitor selector
      ##
      selectorOverride: {}

      ## Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.
      ##
      attachMetadata:
        node: false

      relabelings: []
      metricRelabelings: []
      interval: ""
      scrapeTimeout: 10s
      ## prometheus.monitor.apiVersion ApiVersion for the serviceMonitor Resource(defaults to "monitoring.coreos.com/v1")
      apiVersion: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

    # PodMonitor defines monitoring for a set of pods.
    # ref. https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.PodMonitor
    # Using a PodMonitor may be preferred in some environments where there is very large number
    # of Windows Exporter endpoints (1000+) behind a single service.
    # The PodMonitor is disabled by default. When switching from ServiceMonitor to PodMonitor,
    # the time series resulting from the configuration through PodMonitor may have different labels.
    # For instance, there will not be the service label any longer which might
    # affect PromQL queries selecting that label.
    podMonitor:
      enabled: true
      # Namespace in which to deploy the pod monitor. Defaults to the release namespace.
      namespace: ""
      # Additional labels, e.g. setting a label for pod monitor selector as set in prometheus
      additionalLabels:
        release: monitoring-stack
      # PodTargetLabels transfers labels of the Kubernetes Pod onto the target.
      podTargetLabels: []
      # apiVersion defaults to monitoring.coreos.com/v1.
      apiVersion: ""
      # Override pod selector to select pod objects.
      selectorOverride: {}
      # Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.
      attachMetadata:
        node: false
      # The label to use to retrieve the job name from. Defaults to label app.kubernetes.io/name.
      jobLabel: ""

      # Scheme/protocol to use for scraping.
      scheme: "http"
      # Path to scrape metrics at.
      path: "/metrics"

      # BasicAuth allow an endpoint to authenticate over basic authentication.
      # More info: https://prometheus.io/docs/operating/configuration/#endpoint
      basicAuth: {}
      # Secret to mount to read bearer token for scraping targets.
      # The secret needs to be in the same namespace as the pod monitor and accessible by the Prometheus Operator.
      # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#secretkeyselector-v1-core
      bearerTokenSecret: {}
      # TLS configuration to use when scraping the endpoint.
      tlsConfig: {}
      # Authorization section for this endpoint.
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.SafeAuthorization
      authorization: {}
      # OAuth2 for the URL. Only valid in Prometheus versions 2.27.0 and newer.
      # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.OAuth2
      oauth2: {}

      # ProxyURL eg http://proxyserver:2195. Directs scrapes through proxy to this endpoint.
      proxyUrl: ""
      # Interval at which endpoints should be scraped. If not specified Prometheus’ global scrape interval is used.
      interval: ""
      # Timeout after which the scrape is ended. If not specified, the Prometheus global scrape interval is used.
      scrapeTimeout: ""
      # HonorTimestamps controls whether Prometheus respects the timestamps present in scraped data.
      honorTimestamps: true
      # HonorLabels chooses the metric’s labels on collisions with target labels.
      honorLabels: true
      # Whether to enable HTTP2. Default false.
      enableHttp2: ""
      # Drop pods that are not running. (Failed, Succeeded).
      # Enabled by default. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase
      filterRunning: ""
      # FollowRedirects configures whether scrape requests follow HTTP 3xx redirects. Default false.
      followRedirects: ""
      # Optional HTTP URL parameters
      params: {}

      # RelabelConfigs to apply to samples before scraping. Prometheus Operator automatically adds
      # relabelings for a few standard Kubernetes fields. The original scrape job’s name
      # is available via the __tmp_prometheus_job_name label.
      # More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config
      relabelings: []
      # MetricRelabelConfigs to apply to samples before ingestion.
      metricRelabelings: []

      # SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      sampleLimit: 0
      # TargetLimit defines a limit on the number of scraped targets that will be accepted.
      targetLimit: 0
      # Per-scrape limit on number of labels that will be accepted for a sample.
      # Only valid in Prometheus versions 2.27.0 and newer.
      labelLimit: 0
      # Per-scrape limit on length of labels name that will be accepted for a sample.
      # Only valid in Prometheus versions 2.27.0 and newer.
      labelNameLengthLimit: 0
      # Per-scrape limit on length of labels value that will be accepted for a sample.
      # Only valid in Prometheus versions 2.27.0 and newer.
      labelValueLengthLimit: 0

  ## Customize the updateStrategy if set
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
  #   memory: 30Mi

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
    annotations: {}
    imagePullSecrets: []
    automountServiceAccountToken: false

  securityContext:
    windowsOptions:
      hostProcess: true
      runAsUserName: "NT AUTHORITY\\system"

  containerSecurityContext: {}

  rbac:
    ## If true, create & use RBAC resources
    ##
    create: true

  # Expose the service to the host network
  hostNetwork: true

  # Share the host process ID namespace
  hostPID: true

  ## Assign a group of affinity scheduling rules
  ##
  affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #         - matchFields:
  #             - key: metadata.name
  #               operator: In
  #               values:
  #                 - target-host-name

  # Annotations to be added to windows exporter pods
  podAnnotations:
    # Fix for very slow GKE cluster upgrades
    cluster-autoscaler.kubernetes.io/safe-to-evict: "true"

  # Extra labels to be added to windows exporter pods
  podLabels: {}

  # Annotations to be added to windows exporter daemonset
  daemonsetAnnotations: {}

  ## set to true to add the release label so scraping of the servicemonitor with kube-prometheus-stack works out of the box
  releaseLabel: false

  # Custom DNS configuration to be added to prometheus-windows-exporter pods
  dnsConfig: {}
  # nameservers:
  #   - 1.2.3.4
  # searches:
  #   - ns1.svc.cluster-domain.example
  #   - my.dns.search.suffix
  # options:
  #   - name: ndots
  #     value: "2"
  #   - name: edns0

  ## Assign a nodeSelector if operating a hybrid cluster
  ##
  nodeSelector:
    kubernetes.io/os: windows
    #  kubernetes.io/arch: amd64

  tolerations:
    - effect: NoSchedule
      operator: Exists
    - operator: Exists

  ## Assign a PriorityClassName to pods if set
  # priorityClassName: ""

  ## Additional container arguments
  ##
  extraArgs: []
  #   - --collector.service.services-where
  #   - "Name LIKE 'sql%'"

  ## Additional mounts from the host to windows-exporter container
  ##
  extraHostVolumeMounts: []
  #  - name: <mountName>
  #    hostPath: <hostPath>
  #    mountPath: <mountPath>
  #    readOnly: true|false

  ## Additional configmaps to be mounted.
  ##
  configmaps: []
  # - name: <configMapName>
  #   mountPath: <mountPath>
  secrets: []
  # - name: <secretName>
  #   mountPath: <mountPatch>
  ## Override the deployment namespace
  ##
  namespaceOverride: ""

  ## Additional containers for export metrics to text file
  ##
  sidecars: []
  ##  - name: nvidia-dcgm-exporter
  ##    image: nvidia/dcgm-exporter:1.4.3

  ## Volume for sidecar containers
  ##
  sidecarVolumeMount: []
  ##  - name: collector-textfiles
  ##    mountPath: /run/prometheus
  ##    readOnly: false

  ## Additional mounts from the host to sidecar containers
  ##
  sidecarHostVolumeMounts: []
  #  - name: <mountName>
  #    hostPath: <hostPath>
  #    mountPath: <mountPath>
  #    readOnly: true|false
  #    mountPropagation: None|HostToContainer|Bidirectional

  ## Additional InitContainers to initialize the pod
  ##
  extraInitContainers: []

  ## Liveness probe
  ##
  livenessProbe:
    failureThreshold: 3
    httpGet:
      httpHeaders: []
      path: /health
      scheme: http
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1

  ## Readiness probe
  ##
  readinessProbe:
    failureThreshold: 3
    httpGet:
      httpHeaders: []
      path: /health
      scheme: http
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 1

node-problem-detector:
  enabled: true
  settings:
    # Custom monitor definitions to add to Node Problem Detector - to be
    # mounted at /custom-config. These are in addition to pre-packaged monitor
    # definitions provided within the default docker image available at /config:
    # https://github.com/kubernetes/node-problem-detector/tree/master/config
    # settings.custom_monitor_definitions -- Custom plugin monitor config files
    custom_monitor_definitions: {}
      # docker-monitor-filelog.json: |
      #   {
      #     "plugin": "filelog",
      #     "pluginConfig": {
      #       "timestamp": "^time=\"(\\S*)\"",
      #       "message": "msg=\"([^\n]*)\"",
      #       "timestampFormat": "2006-01-02T15:04:05.999999999-07:00"
      #     },
      #     "logPath": "/var/log/docker.log",
      #     "lookback": "5m",
      #     "bufferSize": 10,
      #     "source": "docker-monitor",
      #     "conditions": [],
      #     "rules": [
      #       {
      #         "type": "temporary",
      #         "reason": "CorruptDockerImage",
      #         "pattern": "Error trying v2 registry: failed to register layer: rename /var/lib/docker/image/(.+) /var/lib/docker/image/(.+): directory not empty.*"
      #       }
      #     ]
      #   }
    # settings.log_monitors -- User-specified custom monitor definitions
    log_monitors:
      - /config/kernel-monitor.json
      - /config/docker-monitor.json
      # An example of activating a custom log monitor definition in
      # Node Problem Detector
      # - /custom-config/docker-monitor-filelog.json
    custom_plugin_monitors: []

    # Any extra arguments to append to node-problem-detector command
    # - "--port 20526"
    extraArgs: []

    # settings.prometheus_address -- Prometheus exporter address
    prometheus_address: 0.0.0.0
    # settings.prometheus_port -- Prometheus exporter port
    prometheus_port: 20257

    # The period at which k8s-exporter does forcibly sync with apiserver
    # settings.heartBeatPeriod -- Syncing interval with API server
    heartBeatPeriod: 5m0s

  logDir:
    # logDir.host -- log directory on k8s host
    host: /var/log/
    # logDir.pod -- log directory in pod (volume mount), use logDir.host if empty
    pod: ""

  image:
    repository: registry.k8s.io/node-problem-detector/node-problem-detector
    tag: v0.8.21
    # image.digest -- the image digest. If given it takes precedence over a given tag.
    digest: ""
    pullPolicy: IfNotPresent

  imagePullSecrets: []

  nameOverride: ""
  fullnameOverride: ""

  rbac:
    create: true
    pspEnabled: false

  # hostNetwork -- Run pod on host network
  # Flag to run Node Problem Detector on the host's network. This is typically
  # not recommended, but may be useful for certain use cases.
  hostNetwork: false
  hostPID: false

  volume:
    localtime:
      type: "FileOrCreate"

  priorityClassName: system-node-critical

  securityContext:
    privileged: true

  resources:     
    requests:
      memory: "50Mi"
    limits:
      memory: "100Mi"


  annotations: {}

  labels: {}

  tolerations:
    - effect: NoSchedule
      operator: Exists

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # Labels to add to the service account
    labels: {}
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:

  affinity: {}

  nodeSelector:
    kubernetes.io/os: linux

  metrics:
    # metrics.enabled -- Expose metrics in Prometheus format with default configuration.
    enabled: false
    # metrics.annotations -- Override all default annotations when `metrics.enabled=true` with specified values.
    annotations: {}
    serviceMonitor:
      enabled: false
      additionalLabels: {}
      additionalRelabelings: []
      metricRelabelings: []
    prometheusRule:
      enabled: false
      defaultRules:
        create: true
        disabled: []
      additionalLabels: {}
      additionalRules: []

  env:
  #  - name: FOO
  #    value: BAR
  #  - name: POD_NAME
  #    valueFrom:
  #      fieldRef:
  #        fieldPath: metadata.name

  extraVolumes: []

  extraVolumeMounts: []

  extraContainers: []

  # updateStrategy -- Manage the daemonset update strategy
  updateStrategy: RollingUpdate
  # maxUnavailable -- The max pods unavailable during an update
  maxUnavailable: 1